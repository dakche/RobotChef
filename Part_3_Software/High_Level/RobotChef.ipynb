{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fe6a9db-51de-4e40-81d9-e7c02f6c6ba2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# RobotChef: EggBot\n",
    "----\n",
    "Man used to eat raw meat.\n",
    "Then we discovered fire.\n",
    "\n",
    "Man used to chase for food.\n",
    "Then we discovered agriculture and cultivation.\n",
    "\n",
    "Man used to cook their own food.\n",
    "Soon we will have robots.\n",
    "\n",
    "Welcome to the future :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c6d8b2-9a6b-4685-acbc-b283dd565833",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 1: Setting Up and Testing Arm Control\n",
    "________________\n",
    "This first part will import the libraries necessary to establish communication between the KR260 and the robot arm.\n",
    "\n",
    "The communication will be via Serial/UART/USB and the communication protocol is pretty simple:\n",
    "\n",
    "- KR260 sends a 'p' followed by a newline ('\\n') --> polls the robot arm joint angles\n",
    "\n",
    "- KR260 commands the arm like this:\n",
    "                \n",
    "                  double; double; double; double; int; int\\n\n",
    "\n",
    "The first four doubles indicate the desired shoulder, elbow, wrist rotation, wrist bend angles, respectively. The last two ints indicate turntable speed and claw servo angle, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b1f9ba3-43f1-40da-8b0a-f8933714f69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import serial\n",
    "import serial.tools.list_ports\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fe01ab-38b6-4b8f-9d57-d5e590dc2d03",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Arm Initialization and Control\n",
    "The function initArm establishes the Serial connection to the Teensy on the robot arm and returns the Serial object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "946e5d2e-5edc-4f87-8b35-2d79b09ea45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the Teensy's USB port\n",
    "def initArm():\n",
    "    ports = list(serial.tools.list_ports.comports())\n",
    "\n",
    "    for p in ports:\n",
    "        if \"USB Serial\" in p.description:\n",
    "            print(p.device)\n",
    "            print(\"Connected!\")\n",
    "            break\n",
    "\n",
    "    # Connect to that port\n",
    "    arm = serial.Serial(port = p.device, baudrate = 115200, timeout = 0.1)\n",
    "    time.sleep(3)\n",
    "    arm.reset_input_buffer()\n",
    "    return arm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9899121f-9232-4a02-9f82-f2d556c0d7bc",
   "metadata": {},
   "source": [
    "This function polls the position of the current position of the robot arm and returns a list containing the current status of the arm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73430439-b670-4656-9580-7bb67c789354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poll the current arm angles\n",
    "def getCurrPos(arm):\n",
    "    arm.write(b\"p\\n\")\n",
    "\n",
    "    # Wait for Teensy's response\n",
    "    while arm.in_waiting == 0:\n",
    "        continue\n",
    "    \n",
    "    # Grab the data\n",
    "    decoded = arm.readline().decode('utf-8')\n",
    "    arm.reset_input_buffer()\n",
    "    resp = decoded.rstrip(\";\\r\\n\").split(\";\")\n",
    "\n",
    "    # Convert strings to floats\n",
    "    curr_pos = np.array(resp).astype(float).tolist()\n",
    "    return curr_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6109dee7-1aeb-404f-907c-eb583e1183ea",
   "metadata": {},
   "source": [
    "This function directly drives the arm to the desired joint angles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5959b512-3647-4cc1-b9cb-ce15938cab20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sends commands angles to arm\n",
    "def driveArm(arm, angles):\n",
    "    command = (\n",
    "        str(angles[0]) + \"; \" \n",
    "        + str(angles[1]) + \"; \" \n",
    "        + str(angles[2]) + \"; \" \n",
    "        + str(angles[3]) + \"; \" \n",
    "        + str(angles[4]) + \"; \"  \n",
    "        + str(angles[5]) + \"\\n\"\n",
    "    )\n",
    "    arm.write(command.encode('ascii'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4e9f2a-d1ff-446a-84eb-5a1a5ccf8a00",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Arm Control Test\n",
    "This is a simple test that moves the arm to three positions and back to ensure that everything is working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a06c0a3a-b959-4d41-a94f-204c776a3cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos1 = [0, 30, 0, 200, 0, 1200]\n",
    "pos3 = [110, 120, 0, 150, 0, 1200]\n",
    "pos2 = [80, 45, 0, 180, 0, 1200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f027e62-4f6e-4156-a2e7-3430fa399ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "arm = initArm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "f4b082f9-a1ce-4b98-a460-8d53f2fbfd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "driveArm(arm, pos1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12357bd-7c6d-4ba3-b520-50465eda811c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(getCurrPos(arm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3492260-f8b5-44c4-abd4-14f63f1bf8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "driveArm(arm, pos1)\n",
    "time.sleep(3)\n",
    "driveArm(arm, pos2)\n",
    "time.sleep(3)\n",
    "driveArm(arm, pos3)\n",
    "time.sleep(3)\n",
    "driveArm(arm, pos2)\n",
    "time.sleep(3)\n",
    "driveArm(arm, pos1)\n",
    "time.sleep(3)\n",
    "driveArm(arm, pos2)\n",
    "time.sleep(3)\n",
    "driveArm(arm, pos3)\n",
    "time.sleep(3)\n",
    "driveArm(arm, pos2)\n",
    "time.sleep(3)\n",
    "driveArm(arm, pos1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b79cc8de-719c-4fd8-9cf7-bce82fee19a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "arm.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a50c09f-da13-4175-ae68-37605b8d7dec",
   "metadata": {},
   "source": [
    "## Part 2: Vision\n",
    "_________________________\n",
    "Goal of this section is to test the camera and run the standard Yolov5 framework on the CPU to test model functionality and camera operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00f6cb7d-8581-4993-bdba-5f939fbaf38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56031cd3-c8b1-4722-8cc7-f39f0e0848d5",
   "metadata": {},
   "source": [
    "### Camera Test\n",
    "Open the camera and load video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc41dc2-0c01-471a-a057-bc19aa17a486",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(-1)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    cv2.imshow('frame', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaaf0b7-e0c6-42a4-8f26-1932a363aed2",
   "metadata": {},
   "source": [
    "We can also take a picture for testing purposes later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21b8f3d-0391-48d2-8aee-d273b8c87ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /root/jupyter_notebooks\n",
    "cap2 = cv2.VideoCapture(-1)\n",
    "ret, frame = cap2.read()\n",
    "\n",
    "while(True):\n",
    "    cv2.imshow('img1', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('y'):\n",
    "        cv2.imwrite('test.png', frame)\n",
    "        cv2.destroyAllWindows()\n",
    "        break\n",
    "cap2.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f813a9-4b57-4139-a778-f632cd8ec298",
   "metadata": {},
   "source": [
    "### Testing non-accelerated Yolov5 with camera input\n",
    "This just downloads the Yolov5 model from Ultralytics and runs detect.py using custom weights and the camera input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e366c7-07fd-4073-86e1-e803588b2c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ultralytics/yolov5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae120c10-00b0-42d7-a44f-91cad1479e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /root/jupyter_notebooks/yolov5\n",
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0f4e9a-1284-4c48-b5d9-89c678c88a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /root/jupyter_notebooks\n",
    "!python3 ./yolov5/detect.py --weights ./yolov5best.pt --img 640 --conf 0.25 --source 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da020a0e-4e23-4bbe-9716-0ce92354c728",
   "metadata": {},
   "source": [
    "### Testing DPU-accelerated Yolov5 with camera input\n",
    "\n",
    "Last time, each frame takes about 3 seconds to process, giving us a total performance of about 0.3 FPS. Let's try using the DPU.\n",
    "\n",
    "This code is modified from the dpu-yolov3 tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf1cc12-ee00-4f1e-9a6a-45eb74fc62a5",
   "metadata": {},
   "source": [
    "First import the overlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d34c27-aa96-4553-9610-e05e1d28a256",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynq_dpu import DpuOverlay\n",
    "overlay = DpuOverlay(\"dpu.bit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e35ac0-be15-455d-8163-b22ce5cb6483",
   "metadata": {},
   "source": [
    "As well as some other libraries we will be using..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d1aac52-f2eb-4e84-9476-fc5e34a7f644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import colorsys\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6a182d-12f1-4245-a20e-abbed318766e",
   "metadata": {},
   "source": [
    "Load the xmodel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13a042e-b677-46e8-bf0f-c348f409dbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlay.load_model(\"yolov5_kr260.xmodel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eeafe1-e08e-4440-aadb-34417253e105",
   "metadata": {},
   "source": [
    "Load the anchor list, this is directly pulled from the anchor list found in yolov5m.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74fd7a9d-f4a2-4ddc-82da-2920c037b2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_list = [10,13,16,30,33,23,30,61,62,45,59,119,116,90,156,198,373,326]\n",
    "anchor_float = [float(x) for x in anchor_list]\n",
    "anchors = np.array(anchor_float).reshape(-1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2538f1-6631-4b6d-9f46-f740cd0ba4a3",
   "metadata": {},
   "source": [
    "Some preprocessing functions, look at comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52bfd955-882b-4fd4-8ff3-ef4dbc0a95ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Get model classification information'''\t\n",
    "import yaml\n",
    "def get_class(classes_path):\n",
    "    with open(classes_path) as f:\n",
    "        data = yaml.safe_load(f)\n",
    "        classes = data.get('names', [])\n",
    "        return classes\n",
    "    \n",
    "classes_path = \"./Egg-Detector-8/data.yaml\"\n",
    "class_names = get_class(classes_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85978774-55e5-44c4-bca3-40c6d79a200d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(class_names)\n",
    "hsv_tuples = [(1.0 * x / num_classes, 1., 1.) for x in range(num_classes)]\n",
    "colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n",
    "colors = list(map(lambda x: \n",
    "                  (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), \n",
    "                  colors))\n",
    "random.seed(0)\n",
    "random.shuffle(colors)\n",
    "random.seed(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b430faa-0f03-4f0c-8fe3-5c3cd344cf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''resize image with unchanged aspect ratio using padding'''\n",
    "def letterbox_image(image, size):\n",
    "    ih, iw, _ = image.shape\n",
    "    w, h = size\n",
    "    scale = min(w/iw, h/ih)\n",
    "    #print(scale)\n",
    "    \n",
    "    nw = int(iw*scale)\n",
    "    nh = int(ih*scale)\n",
    "    #print(nw)\n",
    "    #print(nh)\n",
    "\n",
    "    image = cv2.resize(image, (nw,nh), interpolation=cv2.INTER_LINEAR)\n",
    "    new_image = np.ones((h,w,3), np.uint8) * 128\n",
    "    h_start = (h-nh)//2\n",
    "    w_start = (w-nw)//2\n",
    "    new_image[h_start:h_start+nh, w_start:w_start+nw, :] = image\n",
    "    return new_image\n",
    "\n",
    "\n",
    "'''image preprocessing'''\n",
    "def pre_process(image, model_image_size):\n",
    "    image = image[...,::-1]\n",
    "    image_h, image_w, _ = image.shape\n",
    " \n",
    "    if model_image_size != (None, None):\n",
    "        assert model_image_size[0]%32 == 0, 'Multiples of 32 required'\n",
    "        assert model_image_size[1]%32 == 0, 'Multiples of 32 required'\n",
    "        boxed_image = letterbox_image(image, tuple(reversed(model_image_size)))\n",
    "    else:\n",
    "        new_image_size = (image_w - (image_w % 32), image_h - (image_h % 32))\n",
    "        boxed_image = letterbox_image(image, new_image_size)\n",
    "    image_data = np.array(boxed_image, dtype='float32')\n",
    "    image_data /= 255.\n",
    "    image_data = np.expand_dims(image_data, 0) \t\n",
    "    return image_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0805bca5-4c45-463a-886b-cb87c83523ca",
   "metadata": {},
   "source": [
    "Processes output tensors into something that we can comprehend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c265d8ce-7e81-4b69-b0fa-ea4f1c85be9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_feats(feats, anchors, num_classes, input_shape):\n",
    "    num_anchors = len(anchors)\n",
    "    anchors_tensor = np.reshape(np.array(anchors, dtype=np.float32), [1, 1, 1, num_anchors, 2])\n",
    "    grid_size = np.shape(feats)[1:3]\n",
    "    nu = num_classes + 5\n",
    "    predictions = np.reshape(feats, [-1, grid_size[0], grid_size[1], num_anchors, nu])\n",
    "    grid_y = np.tile(np.reshape(np.arange(grid_size[0]), [-1, 1, 1, 1]), [1, grid_size[1], 1, 1])\n",
    "    grid_x = np.tile(np.reshape(np.arange(grid_size[1]), [1, -1, 1, 1]), [grid_size[0], 1, 1, 1])\n",
    "    grid = np.concatenate([grid_x, grid_y], axis = -1)\n",
    "    grid = np.array(grid, dtype=np.float32)\n",
    "\n",
    "    box_xy = (1/(1+np.exp(-predictions[..., :2])) + grid) / np.array(grid_size[::-1], dtype=np.float32)\n",
    "    box_wh = np.exp(predictions[..., 2:4]) * anchors_tensor / np.array(input_shape[::-1], dtype=np.float32)\n",
    "    box_confidence = 1/(1+np.exp(-predictions[..., 4:5]))\n",
    "    box_class_probs = 1/(1+np.exp(-predictions[..., 5:]))\n",
    "    return box_xy, box_wh, box_confidence, box_class_probs\n",
    "\n",
    "\n",
    "def correct_boxes(box_xy, box_wh, input_shape, image_shape):\n",
    "    box_yx = box_xy[..., ::-1]\n",
    "    box_hw = box_wh[..., ::-1]\n",
    "    input_shape = np.array(input_shape, dtype = np.float32)\n",
    "    image_shape = np.array(image_shape, dtype = np.float32)\n",
    "    new_shape = np.around(image_shape * np.min(input_shape / image_shape))\n",
    "    offset = (input_shape - new_shape) / 2. / input_shape\n",
    "    scale = input_shape / new_shape\n",
    "    box_yx = (box_yx - offset) * scale\n",
    "    box_hw *= scale\n",
    "\n",
    "    box_mins = box_yx - (box_hw / 2.)\n",
    "    box_maxes = box_yx + (box_hw / 2.)\n",
    "    boxes = np.concatenate([\n",
    "        box_mins[..., 0:1],\n",
    "        box_mins[..., 1:2],\n",
    "        box_maxes[..., 0:1],\n",
    "        box_maxes[..., 1:2]\n",
    "    ], axis = -1)\n",
    "    boxes *= np.concatenate([image_shape, image_shape], axis = -1)\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def boxes_and_scores(feats, anchors, classes_num, input_shape, image_shape):\n",
    "    box_xy, box_wh, box_confidence, box_class_probs = _get_feats(feats, anchors, classes_num, input_shape)\n",
    "    boxes = correct_boxes(box_xy, box_wh, input_shape, image_shape)\n",
    "    boxes = np.reshape(boxes, [-1, 4])\n",
    "    box_scores = box_confidence * box_class_probs\n",
    "    box_scores = np.reshape(box_scores, [-1, classes_num])\n",
    "    return boxes, box_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab9523ed-5128-48cb-a201-60f4c7f13b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Draw detection frame'''\n",
    "def draw_bbox(image, bboxes, classes):\n",
    "    \"\"\"\n",
    "    bboxes: [x_min, y_min, x_max, y_max, probability, cls_id] format coordinates.\n",
    "    \"\"\"\n",
    "    num_classes = len(classes)\n",
    "    image_h, image_w, _ = image.shape\n",
    "    hsv_tuples = [(1.0 * x / num_classes, 1., 1.) for x in range(num_classes)]\n",
    "    colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n",
    "    colors = list(map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), colors))\n",
    "\n",
    "    random.seed(0)\n",
    "    random.shuffle(colors)\n",
    "    random.seed(None)\n",
    "\n",
    "    for i, bbox in enumerate(bboxes):\n",
    "        coor = np.array(bbox[:4], dtype=np.int32)\n",
    "        fontScale = 0.5\n",
    "        score = bbox[4]\n",
    "        class_ind = int(bbox[5])\n",
    "        bbox_color = colors[class_ind]\n",
    "        bbox_thick = int(0.6 * (image_h + image_w) / 600)\n",
    "        c1, c2 = (coor[0], coor[1]), (coor[2], coor[3])\n",
    "        cv2.rectangle(image, c1, c2, bbox_color, bbox_thick)\n",
    "    return image\n",
    "\n",
    "\n",
    "def nms_boxes(boxes, scores):\n",
    "    \"\"\"Suppress non-maximal boxes.\n",
    "\n",
    "    # Arguments\n",
    "        boxes: ndarray, boxes of objects.\n",
    "        scores: ndarray, scores of objects.\n",
    "\n",
    "    # Returns\n",
    "        keep: ndarray, index of effective boxes.\n",
    "    \"\"\"\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 2]\n",
    "    y2 = boxes[:, 3]\n",
    "\n",
    "    areas = (x2-x1+1)*(y2-y1+1)\n",
    "    order = scores.argsort()[::-1]\n",
    "\n",
    "    keep = []\n",
    "    while order.size > 0:\n",
    "        i = order[0]\n",
    "        keep.append(i)\n",
    "\n",
    "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "\n",
    "        w1 = np.maximum(0.0, xx2 - xx1 + 1)\n",
    "        h1 = np.maximum(0.0, yy2 - yy1 + 1)\n",
    "        inter = w1 * h1\n",
    "\n",
    "        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n",
    "        inds = np.where(ovr <= 0.45)[0]  # threshold\n",
    "        order = order[inds + 1]\n",
    "\n",
    "    return keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07e9ff90-b707-413a-8073-da464db0b245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(image, boxes, scores, classes):\n",
    "    _, ax = plt.subplots(1)\n",
    "    ax.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    image_h, image_w, _ = image.shape\n",
    "\n",
    "    for i, bbox in enumerate(boxes):\n",
    "        [top, left, bottom, right] = bbox\n",
    "        width, height = right - left, bottom - top\n",
    "        center_x, center_y = left + width*0.5, top + height*0.5\n",
    "        score, class_index = scores[i], classes[i]\n",
    "        label = '{}: {:.4f}'.format(class_names[class_index], score) \n",
    "        color = tuple([color/255 for color in colors[class_index]])\n",
    "        ax.add_patch(Rectangle((left, top), width, height,\n",
    "                               edgecolor=color, facecolor='none'))\n",
    "        ax.annotate(label, (center_x, center_y), color=color, weight='bold', \n",
    "                    fontsize=12, ha='center', va='center')\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a89d7c2c-2702-4243-a028-d6ab22c39895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(yolo_outputs, image_shape, class_names, anchors):\n",
    "    score_thresh = 0.1\n",
    "    anchor_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]\n",
    "    boxes = []\n",
    "    box_scores = []\n",
    "    input_shape = np.shape(yolo_outputs[0])[1 : 3]\n",
    "    input_shape = np.array(input_shape)*32\n",
    "\n",
    "    for i in range(len(yolo_outputs)):\n",
    "        _boxes, _box_scores = boxes_and_scores(\n",
    "            yolo_outputs[i], anchors[anchor_mask[i]], len(class_names), \n",
    "            input_shape, image_shape)\n",
    "        boxes.append(_boxes)\n",
    "        box_scores.append(_box_scores)\n",
    "    boxes = np.concatenate(boxes, axis = 0)\n",
    "    box_scores = np.concatenate(box_scores, axis = 0)\n",
    "\n",
    "    mask = box_scores >= score_thresh\n",
    "    boxes_ = []\n",
    "    scores_ = []\n",
    "    classes_ = []\n",
    "    for c in range(len(class_names)):\n",
    "        class_boxes_np = boxes[mask[:, c]]\n",
    "        class_box_scores_np = box_scores[:, c]\n",
    "        class_box_scores_np = class_box_scores_np[mask[:, c]]\n",
    "        nms_index_np = nms_boxes(class_boxes_np, class_box_scores_np) \n",
    "        class_boxes_np = class_boxes_np[nms_index_np]\n",
    "        class_box_scores_np = class_box_scores_np[nms_index_np]\n",
    "        classes_np = np.ones_like(class_box_scores_np, dtype = np.int32) * c\n",
    "        boxes_.append(class_boxes_np)\n",
    "        scores_.append(class_box_scores_np)\n",
    "        classes_.append(classes_np)\n",
    "    boxes_ = np.concatenate(boxes_, axis = 0)\n",
    "    scores_ = np.concatenate(scores_, axis = 0)\n",
    "    classes_ = np.concatenate(classes_, axis = 0)\n",
    "\n",
    "    return boxes_, scores_, classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea4e8bb-5eb3-4f1b-b563-c65a3203447d",
   "metadata": {},
   "source": [
    "Now we will use VART to run the model on the DPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "085b2218-894b-4ea0-8a9b-19fc03c58629",
   "metadata": {},
   "outputs": [],
   "source": [
    "dpu = overlay.runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14c78795-b634-4e82-b92a-03a39c2554f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputTensors = dpu.get_input_tensors()\n",
    "outputTensors = dpu.get_output_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb3baad1-70be-4a8a-a523-cc762ea43c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapeIn = tuple(inputTensors[0].dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce50728e-5de5-44f5-8c94-9bfece489006",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapeOut0 = (tuple(outputTensors[0].dims))\n",
    "shapeOut1 = (tuple(outputTensors[1].dims))\n",
    "shapeOut2 = (tuple(outputTensors[2].dims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8a6c6f8-cb77-4366-b70c-50db38dce958",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputSize0 = int(outputTensors[0].get_data_size() / shapeIn[0])\n",
    "outputSize1 = int(outputTensors[1].get_data_size() / shapeIn[0])\n",
    "outputSize2 = int(outputTensors[2].get_data_size() / shapeIn[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "580f1405-12f7-47d2-8851-b913e917891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = [np.empty(shapeIn, dtype=np.float32, order=\"C\")]\n",
    "output_data = [np.empty(shapeOut0, dtype=np.float32, order=\"C\"), \n",
    "               np.empty(shapeOut1, dtype=np.float32, order=\"C\"),\n",
    "               np.empty(shapeOut2, dtype=np.float32, order=\"C\")]\n",
    "image = input_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8178ae1d-3043-43b9-b73c-aad77bc50dea",
   "metadata": {},
   "source": [
    "Cluster finding code (pseudo Kruskal's Algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b50ff5e-4628-46c9-a2de-254e018912d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates distance between two points\n",
    "def dist(p1, p2):\n",
    "    x1 = p1[0]\n",
    "    y1 = p1[1]\n",
    "    x2 = p2[0]\n",
    "    y2 = p2[1]\n",
    "    return np.sqrt((x2-x1)**2 + (y2 - y1)**2)\n",
    "\n",
    "def find(parent, i):\n",
    "    if parent[i] == i:\n",
    "        return i\n",
    "    return find(parent, parent[i])\n",
    "\n",
    "def union(parent, rank, x, y):\n",
    "    xroot = find(parent, x)\n",
    "    yroot = find(parent, y)\n",
    "\n",
    "    if rank[xroot] < rank[yroot]:\n",
    "        parent[xroot] = yroot\n",
    "    elif rank[xroot] > rank[yroot]:\n",
    "        parent[yroot] = xroot\n",
    "    else:\n",
    "        parent[yroot] = xroot\n",
    "        rank[xroot] += 1\n",
    "\n",
    "# Calculate centroid of a set of points\n",
    "def centroid(pts):\n",
    "    point_x = [point[0] for point in pts]\n",
    "    point_y = [point[1] for point in pts]\n",
    "    \n",
    "    centroid_x = np.mean(point_x)\n",
    "    centroid_y = np.mean(point_y)\n",
    "    \n",
    "    return (centroid_x, centroid_y)\n",
    "        \n",
    "        \n",
    "# Gets clusters from a set of points using Union Find\n",
    "def getClusters(pts, thresh):\n",
    "    \n",
    "    #print(\"Pts: \", pts)\n",
    "    distances = []\n",
    "    for i in range(len(pts)):\n",
    "        for j in range(i + 1, len(pts)):\n",
    "            distance = dist(pts[i], pts[j])\n",
    "            if (distance <= thresh):\n",
    "                distances.append((distance, pts[i], pts[j]))\n",
    "    \n",
    "    distances_ = sorted(distances, key=lambda x: x[0])\n",
    "    \n",
    "    # Initialize parent and rank arrays for union-find\n",
    "    parent = list(range(len(pts)))\n",
    "    rank = [0] * len(pts)\n",
    "\n",
    "    # Perform union-find on nearby points\n",
    "    for _, point1, point2 in distances_:\n",
    "        point1_index = np.where((pts == point1).all(axis=1))[0][0]\n",
    "        point2_index = np.where((pts == point2).all(axis=1))[0][0]\n",
    "        union(parent, rank, point1_index, point2_index)\n",
    "\n",
    "    # Find the connected components (clusters)\n",
    "    clusters = {}\n",
    "    for i in range(len(pts)):\n",
    "        root = find(parent, i)\n",
    "        if root not in clusters:\n",
    "            clusters[root] = []\n",
    "        clusters[root].append(i)\n",
    "\n",
    "    # Only save clusters that are bigger than 5 elements\n",
    "    new_clusters = {}\n",
    "    for cluster_root, points_indices in clusters.items():\n",
    "        if len(points_indices) >= 5:\n",
    "            new_clusters[cluster_root] = points_indices\n",
    "            \n",
    "    eggs = []\n",
    "    # Get centroids of the clusters\n",
    "    for cluster_root, points_indices in new_clusters.items():\n",
    "        eggs.append(centroid([pts[i] for i in points_indices]))\n",
    "        \n",
    "    return eggs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851716a8-5a54-4914-a747-075757fd5b92",
   "metadata": {},
   "source": [
    "Our main function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b31d5e7-9afe-430d-98de-e1ba9ceadfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runYolo():\n",
    "    cap = cv2.VideoCapture(-1)\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, input_image = cap.read()\n",
    "        # input_image = cv2.cvtColor(frame)\n",
    "        time1 = time.time()\n",
    "        # Pre-processing\n",
    "        image_size = input_image.shape[:2]\n",
    "        image_data = np.array(pre_process(input_image, (640, 640)), dtype=np.float32)\n",
    "    \n",
    "        # Fetch data to DPU and trigger it\n",
    "        image[0,...] = image_data.reshape(shapeIn[1:])\n",
    "        job_id = dpu.execute_async(input_data, output_data)\n",
    "        dpu.wait(job_id)\n",
    "\n",
    "        # Retrieve output data\n",
    "        conv_out0 = np.reshape(output_data[0], shapeOut0)\n",
    "        conv_out1 = np.reshape(output_data[1], shapeOut1)\n",
    "        conv_out2 = np.reshape(output_data[2], shapeOut2)\n",
    "        yolo_outputs = [conv_out0, conv_out1, conv_out2]\n",
    "\n",
    "        # Decode output from YOLOv3\n",
    "        boxes, _, classes = evaluate(yolo_outputs, image_size, class_names, anchors)\n",
    "\n",
    "        image_h, image_w, _ = input_image.shape\n",
    "        \n",
    "        # Init vars for calculating centroids\n",
    "        egg_pts = []\n",
    "        count_eggs = 0\n",
    "\n",
    "        sumX_strainer = 0\n",
    "        sumY_strainer = 0\n",
    "        count_strainer = 0\n",
    "\n",
    "        sumX_handle = 0\n",
    "        sumY_handle = 0\n",
    "        count_handle = 0\n",
    "\n",
    "        for i, bbox in enumerate(boxes):\n",
    "            [top, left, bottom, right] = bbox\n",
    "            width, height = right - left, bottom - top\n",
    "            center_x, center_y = left + width*0.5, top + height*0.5\n",
    "            class_index = classes[i]\n",
    "            \n",
    "            # Grabs the center of each box found\n",
    "            # For the sake of this demonstration, we assume there is only one strainer and one handle\n",
    "            # But the number of eggs is unknown, so we will run a slightly different algorithm\n",
    "            if (class_index == 0):\n",
    "                egg_pts.append([center_x, center_y])\n",
    "                count_eggs += 1\n",
    "            elif (class_index == 1):\n",
    "                sumX_strainer += center_x\n",
    "                sumY_strainer += center_y\n",
    "                count_strainer += 1\n",
    "            elif (class_index == 2):\n",
    "                sumX_handle += center_x\n",
    "                sumY_handle += center_y\n",
    "                count_handle += 1\n",
    "        \n",
    "        # Circles the average of all boxes\n",
    "        if (count_eggs > 0):\n",
    "            all_pts = np.array(egg_pts)\n",
    "            clusters = getClusters(all_pts, 20)\n",
    "            colorEgg = tuple([color for color in colors[0]])\n",
    "            for x_coor, y_coor in clusters:\n",
    "                cv2.circle(input_image, (int(x_coor), int(y_coor)), radius = 5, color = colorEgg, thickness = 1)\n",
    "        \n",
    "        if (count_strainer > 0):\n",
    "            ctrX_Str = int(sumX_strainer / count_strainer)\n",
    "            ctrY_Str = int(sumY_strainer / count_strainer)\n",
    "            colorStr = tuple([color for color in colors[1]])\n",
    "            cv2.circle(input_image, (ctrX_Str, ctrY_Str), radius = 5, color = colorStr, thickness = 1)\n",
    "        if (count_handle > 0):\n",
    "            ctrX_Han = int(sumX_handle / count_handle)\n",
    "            ctrY_Han = int(sumY_handle / count_handle)\n",
    "            colorHan = tuple([color for color in colors[2]])\n",
    "            cv2.circle(input_image, (ctrX_Han, ctrY_Han), radius = 5, color = colorHan, thickness = 1)\n",
    "            \n",
    "        time2 = time.time()\n",
    "        fps = 1/(time2-time1)\n",
    "        fpsStats = 'FPS: {:.2f}'.format(fps)\n",
    "        cv2.putText(input_image, fpsStats, (0,30), cv2.FONT_HERSHEY_PLAIN, 2, (255, 255, 0), 2)\n",
    "        cv2.imshow('frame', input_image)\n",
    "            \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32541e72-7119-4b07-b70a-feb925359f2b",
   "metadata": {},
   "source": [
    "Run the test! We get about 6 FPS, almost a 20x speedup!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3d48134-cb04-40d3-9a31-20ff6eabfe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "runYolo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6667db2-c43f-467e-a99c-1c01b3e76e7a",
   "metadata": {},
   "source": [
    "## DON'T FORGET TO RELEASE THE DPU WHEN YOU ARE DONE WITH IT!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33e5f9bd-fed0-4a2b-90e9-4f03e88dfeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "del overlay\n",
    "del dpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deead969-5d8c-42d1-9855-62e5e6487b9b",
   "metadata": {},
   "source": [
    "# Part 3: Implementing OCR\n",
    "_______\n",
    "Reading the buttons on the stove! Due to time constraints, this part is not accelerated on the DPU, though it may be in the near future. TesseractOCR is used because it is CPU-friendly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fd127d-321c-4640-b68b-367c681df5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt install -y tesseract-ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f07bf90-220d-4ac9-9657-417470c6f57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8722810c-257c-4359-b32e-40bf02a7cc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "57fab5b3-dc69-42aa-a399-c0a8219c2fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from pytesseract import Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f7935116-7eb0-4d6c-87b0-29c55b503913",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grayscale(image):\n",
    "    return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "def thresholding(image):\n",
    "    return cv2.threshold(image, 200, 255, cv2.THRESH_BINARY)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1309d4-0861-40a6-9116-78f88ae8de5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(-1)\n",
    "cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    gray = get_grayscale(frame)\n",
    "    thresh = thresholding(gray)\n",
    "    imagem = cv2.bitwise_not(thresh)\n",
    "    \n",
    "    d = pytesseract.image_to_data(imagem, output_type=Output.DICT)\n",
    "    n_boxes = len(d['text'])\n",
    "    for i in range(n_boxes):\n",
    "        if int(d['conf'][i]) > 40:\n",
    "            (text, x, y, w, h) = (d['text'][i], d['left'][i], d['top'][i], d['width'][i], d['height'][i])\n",
    "            # don't show empty text\n",
    "            if text and text.strip() != \"\":\n",
    "                imagem = cv2.rectangle(imagem, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                imagem = cv2.putText(imagem, text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 3)\n",
    "    \n",
    "    cv2.imshow('frame', imagem)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5c547b6c-db54-47f5-8ad7-85fbed2ebb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce43527-6774-47fd-abc4-7249091213fa",
   "metadata": {},
   "source": [
    "# Part 4: Forward and Inverse Kinematics\n",
    "----\n",
    "ROS-less implementation from scratch, here we go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff406390-0a4c-4435-a970-a78054b3f3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d28abf1-1ceb-49e3-9603-b36d50cabbc5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define Robot Arm Features\n",
    "These constants define the physical dimensions of the robot arm for the inverse kinematics calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dfe26e4-a5da-4f1c-b514-5f41b8ba876c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Base height to shoulder pivot in mm\n",
    "L_ZOFFSET = 311\n",
    "\n",
    "# Base X-distance offset from shoulder pivot in mm\n",
    "L_XOFFSET = 124\n",
    "\n",
    "# Upper arm length in mm (shoulder pivot --> elbow pivot)\n",
    "L_UARM = 260\n",
    "\n",
    "# Upper arm Angle offset in deg (Upper arm --> Elbow point)\n",
    "A_UOFFSET = 31.7\n",
    "\n",
    "# Forearm length in mm (elbow pivot --> wrist pivot)\n",
    "L_4ARM = 274\n",
    "\n",
    "# Hand Length in mm (wrist pivot --> fingers/grabbing location)\n",
    "L_HAND = 150\n",
    "\n",
    "# Wrist pivot to camera X distance in mm\n",
    "CAM_XDIST = 89\n",
    "\n",
    "# Wrist_pivot to camera Y distance in mm\n",
    "CAM_YDIST = 63.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c48ac3-6ec0-4858-9054-9e525cd3c05b",
   "metadata": {},
   "source": [
    "Since I don't have depth sensing capabilities, I will have to cheat a little and predefine approximate heights of my objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adf91eaf-64b1-47f8-bc16-e4dc804e8724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Height of Egg (to center) in mm\n",
    "H_Egg = 35\n",
    "\n",
    "# Height of Buttons in mm\n",
    "H_Button = 86\n",
    "\n",
    "# Height of strainer in mm\n",
    "H_Strainer = 229\n",
    "\n",
    "# Height of handle in mm\n",
    "H_Handle = 280"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b183ab23-2471-4d73-be00-584053c0c789",
   "metadata": {},
   "source": [
    "Time to do some math. Well, quite a lot of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c9a432c-44d9-471c-adef-18475b1adb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAD2DEG = 180.0 / math.pi\n",
    "DEG2RAD = math.pi / 180.0\n",
    "# Calculates inverse law of cosines\n",
    "# Input triangle side lengths a, b, c\n",
    "# Output angle value for the angle opposite of side c\n",
    "def inv_lawOfCosines(a, b, c):\n",
    "    return ((math.acos(((a**2)+(b**2)-(c**2))/(2*a*b))) * RAD2DEG)\n",
    "\n",
    "def lawOfCosines(a, b, C):\n",
    "    return math.sqrt((a**2)+(b**2)-(2*a*b*math.cos(C*DEG2RAD)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b9d1556-52e8-4d41-8945-b7599fedabd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse Kinematics solver. \n",
    "# Input a 2D location in space with respect to the base of the robot\n",
    "# Input the target device (claw or camera)\n",
    "# Also input desired camera or claw angle, business end wrt the ground\n",
    "# Output is an array of joint angles\n",
    "def IK(loc, device, angle):\n",
    "    # Reference location (aka where the shoulder joint is)\n",
    "    ref = [-L_XOFFSET, L_ZOFFSET]\n",
    "    target = loc\n",
    "    \n",
    "    # Compensating desired position based on device\n",
    "    if (device == 'cam'):\n",
    "        target[0] = target[0] - (CAM_XDIST * math.cos(angle * DEG2RAD)) - (CAM_YDIST * math.sin(angle * DEG2RAD))\n",
    "        target[1] = target[1] - (CAM_XDIST * math.sin(angle * DEG2RAD)) + (CAM_YDIST * math.cos(angle * DEG2RAD))\n",
    "        \n",
    "    elif (device == 'claw'):\n",
    "        target[0] -= L_HAND * math.sin(angle * DEG2RAD)\n",
    "        target[1] += L_HAND * math.cos(angle * DEG2RAD)\n",
    "    \n",
    "    # Distance calculation\n",
    "    v_side = dist(target, ref)\n",
    "    v_angle = 180 - math.asin((target[1]-L_ZOFFSET) / v_side) * RAD2DEG\n",
    "    \n",
    "    # Shoulder angle\n",
    "    virt_s_angle = v_angle - inv_lawOfCosines(v_side, L_UARM, L_4ARM)\n",
    "    s_angle = virt_s_angle - A_UOFFSET\n",
    "    \n",
    "    # Elbow angle\n",
    "    virt_e_angle = inv_lawOfCosines(L_4ARM, L_UARM, v_side)\n",
    "    e_angle = virt_e_angle - A_UOFFSET\n",
    "    \n",
    "    # Wrist bend angle\n",
    "    y_angle = 180 + angle - (virt_e_angle - (virt_s_angle - 90))\n",
    "    \n",
    "    if (device == 'cam'):\n",
    "        y_angle += 90\n",
    "\n",
    "    return [s_angle, e_angle, 0, y_angle, 0, 1500]\n",
    "    \n",
    "# Forward Kinematics solver.\n",
    "# Input joint angles (pos)\n",
    "# Also input desired device\n",
    "# Returns point in space and device angle wrt the ground\n",
    "def FK(pos, device):\n",
    "    s_angle = pos[0]\n",
    "    e_angle = pos[1]\n",
    "    y_angle = pos[3]\n",
    "    \n",
    "    if (device == 'cam'):\n",
    "        y_angle -= 90\n",
    "        \n",
    "    virt_e_angle = e_angle + A_UOFFSET\n",
    "    virt_s_angle = s_angle + A_UOFFSET\n",
    "    \n",
    "    v_side = lawOfCosines(L_4ARM, L_UARM, virt_e_angle)\n",
    "    v_angle = virt_s_angle + inv_lawOfCosines(v_side, L_UARM, L_4ARM)\n",
    "    \n",
    "    angle = y_angle - 180 + (virt_e_angle - (virt_s_angle - 90))\n",
    "    \n",
    "    ypos = (math.sin((180 - v_angle) * DEG2RAD) * v_side) + L_ZOFFSET\n",
    "    \n",
    "    xpos = (v_side * math.cos((180 - v_angle) * DEG2RAD)) - L_XOFFSET\n",
    "    \n",
    "    if (device == 'cam'):\n",
    "        xpos = xpos + (CAM_XDIST * math.cos(angle * DEG2RAD)) + (CAM_YDIST * math.sin(angle * DEG2RAD))\n",
    "        ypos = ypos + (CAM_XDIST * math.sin(angle * DEG2RAD)) - (CAM_YDIST * math.cos(angle * DEG2RAD))\n",
    "        \n",
    "    elif (device == 'claw'):\n",
    "        xpos += L_HAND * math.sin(angle * DEG2RAD)\n",
    "        ypos -= L_HAND * math.cos(angle * DEG2RAD)\n",
    "        \n",
    "    return [xpos, ypos, angle]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee9746b-a196-4d03-ba8a-8ea7b6048be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "arm = initArm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ae75ea-42a1-4fd9-ab4a-60b055fbe535",
   "metadata": {},
   "source": [
    "IK/FK test, drive it to some known position, measure it in real life, and then use FK to report the position back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22433701-b1f7-4b56-97ef-cc7f94a82e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "position = [305, 305]\n",
    "yeet = IK(position, 'cam', 0)\n",
    "print(yeet)\n",
    "driveArm(arm, yeet)\n",
    "time.sleep(2)\n",
    "hm = getCurrPos(arm)\n",
    "print(hm)\n",
    "data = FK(hm, 'cam')\n",
    "print(\"Position: [{}, {}]\".format(data[0], data[1]))\n",
    "print(\"Angle: \", data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22849ea-9e4b-45cf-8b11-662ba0f01e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(getCurrPos(arm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "07d3d7a5-ae3c-4088-8cb1-c5cae07d16b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "arm.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4514e0-9b4c-4d4a-931d-0cc02144067e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
